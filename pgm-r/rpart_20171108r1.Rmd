

```{r}
## 使用ライブラリ
library(dplyr)
library(rpart)
library(pROC)
library(ggplot2)
library(partykit)

## データ読込
train<-read.csv("../motodata/train.csv", header=TRUE)
test<-read.csv("../motodata/test.csv", header=TRUE)

## train, testの結合
test$y <- 9
combi <- rbind(train,test)
```

# データ加工

## jobのグルーピング  
```{r}
combi <- combi %>%
    dplyr::mutate(job2 = if_else(job %in% c('retired','students'), 'notworker', 'worker')) %>%
    dplyr::mutate(job2 = if_else(job == 'unknown' , 'unknown', job2)) %>%
    dplyr::mutate(job2 = as.factor(job2)) %>%
    glimpse

combi <- combi %>%
    dplyr::mutate(job3 = if_else(job %in% c('admin.','bule-collar','management','services','technician'), 'major', 'minor')) %>%
    dplyr::mutate(job3 = as.factor(job3)) %>%
    glimpse
```
## 連続データを線形性を持つように加工
	
* 最終接触時間(duration)は外れ値を0.995%tileを寄せる。線形にするため、ルートを取る。
* 年齢(age)は、50で折り返し。
* 年間平均残高(balance)は、95%tileを取る。
```{r}  
combi <- combi %>%
    dplyr::mutate(duration2=ifelse(duration >= quantile(duration,probs=.995),
                                   quantile(duration,probs=.995),
                                   duration)) %>%
    dplyr::mutate(duration3 = sqrt(duration2)) %>%
    dplyr::mutate(age2=abs(50-age)) %>%
    dplyr::mutate(balance2=ifelse(balance >= quantile(balance,probs=.95),
                                  quantile(balance,probs=.95),
                                  balance))
```
## 前キャンペーンの日付求める
```{r}
lct <- Sys.getlocale("LC_TIME"); Sys.setlocale("LC_TIME", "C")
combi <- combi %>%
    dplyr::mutate(lastdate =
                      as.POSIXct(dplyr::if_else(pdays > 0,
                                                as.Date(paste(day,month,"2017",sep=""),"%d%b%Y"),
                                                as.Date(NA)))) %>%
    dplyr::mutate(pdate =
                      as.POSIXct(dplyr::if_else(pdays > 0,
                                                as.Date(paste(day,month,"2017",sep=""),"%d%b%Y") - pdays,
                                                as.Date(NA))))
Sys.setlocale("LC_TIME", lct)

combi <- combi %>%
    dplyr::mutate(total_contacts = campaign + previous) %>%
    dplyr::mutate(rate_campaign = campaign / total_contacts) %>%
    dplyr::mutate(contact_avg_time = duration / total_contacts) 

combi <- combi %>%
    dplyr::mutate(housing = dplyr::if_else(housing == "yes",1,0)) %>%
    dplyr::mutate(loan = dplyr::if_else(loan == "yes",1,0)) %>%
    glimpse

combi <- combi %>%
    dplyr::mutate(debt = dplyr::if_else(housing + loan > 0, 1, 0)) %>%
    glimpse
```


## trainとtestに分割
```{r}
train <- combi %>%
    dplyr::filter(y < 9)

str(train)

test <- combi %>%
    dplyr::filter(y == 9) %>%
    dplyr::select(-y)

str(test)
```


#### パラメータのチューニング
## 精度を見たいのでホールドアウト法により構築データと検証データに分ける
```{r}
## 構築データの割合
rate<-0.7

## 構築データ数(小数の切捨て)
num<-as.integer(nrow(train)*rate)

########ランダムに構築データを取得########
## 再現性のため乱数シードを固定
set.seed(17)

## sample(ベクトル, ランダムに取得する個数, 復元抽出の有無)
row<-sample(1:nrow(train), num, replace=FALSE)

## 構築データ
train_train<-train[row,]

## 検証データ
train_test<- train[-row,]

## 比較用に普通に構築
## 決定木作成
tree_tmp<-rpart(y~., data=dplyr::select(train_train, -c(id,age2,month,day,duration2,duration3,balance2,pdays)),
                maxdepth=10, minbucket=12, cp=0.000008,
                method="class", parms=list(split="gini"))

tree_tmp<-rpart(y~., data=dplyr::select(train_train, -c(id,age2,month,day,duration2,duration3,balance2,pdays)),
                maxdepth=10, minbucket=10, cp=0.0001,
                method="class", parms=list(split="gini"))

## 検証データへ当てはめ
pred_test<-predict(tree_tmp, dplyr::select(train_test, -c(id,age2,month,day,duration2,duration3,balance2,pdays,y)))[,2]

## AUCの計算
## roc(目的変数(1 or 0), 予測結果)
auc<-roc(train_test$y, pred_test)
print(auc)

plot(as.party(tree_tmp))

###ロジスティック回帰モデル構築
logi_model <- glm(
  y ~ .,    ##目的変数と説明変数の指定(全て使う場合はy~.)
  data = dplyr::select(train_train, -c(id,duration2,duration,age,balance,pdays,job2,job3,education,lastdate)),
  family=binomial(link="logit") ##ロジスティック回帰を指定
)

logi_model <- glm(
  y ~ .,    ##目的変数と説明変数の指定(全て使う場合はy~.)
  data = train_train,
  family=binomial(link="logit") ##ロジスティック回帰を指定
)

##モデルの中身を見る
summary(logi_model)

##モデルの当てはめ
pred_train_test<- predict(logi_model, newdata=train_test, type="response")

##AUC確認
auc<-roc(train_test$y, pred_train_test)$auc
auc
```

## スタッキングの実装
# 今回は決定木(rpart)とロジスティック回帰(glm)をロジスティック回帰(glm)でアンサンブル

```{r}
### Old
#再現性のため乱数シードを固定
set.seed(17)
#学習データをK個にグループ分け
K<-5
#sample(ベクトル, ランダムに取得する個数, 復元抽出の有無, ベクトルの各要素が抽出される確率)
train_old$cv_group<-sample(1:K, nrow(train_old), replace=TRUE, prob=rep(1/K, K))

#構築, 検証データ予測スコアの初期化
score_train_tree<-NULL
score_train_logi<-NULL
score_test_tree<-NULL
score_test_logi<-NULL
y<-NULL

#クロスバリデーション
for(j in 1:K){
  #構築, 検証データに分ける
  train_tmp<-train_old %>%
    dplyr::filter(cv_group!=j) %>%
    dplyr::select(-cv_group)
  test_tmp<-train_old %>%
    dplyr::filter(cv_group==j) %>%
    dplyr::select(-cv_group)

  ## 構築データでモデル構築(決定木)
  tree_tmp<-rpart(y~., data=dplyr::select(train_tmp, -c(id,age2,month,day,duration2,duration3,balance2,pdays)),
                  maxdepth=10, minbucket=12, cp=0.000008,
                  method="class", parms=list(split="gini"))
  
  ## 構築データでモデル構築(ロジスティック回帰)
  logi_tmp<-glm(y~., data=dplyr::select(train_tmp, -c(id,duration2,duration,age,balance,pdays,job2,job3,education,lastdate)),
                family=binomial(link="logit"))

  #モデル構築に使用していないデータの予測値と目的変数
  pred_train_tree<-predict(tree_tmp, test_tmp)[,2]
  pred_train_logi<-predict(logi_tmp, test_tmp, type="response")
  y<-c(y, test_tmp$y)
  
  score_train_tree<-c(score_train_tree, pred_train_tree)
  score_train_logi<-c(score_train_logi, pred_train_logi)
  
  
  #検証データの予測値
  pred_test_tree<-predict(tree_tmp, test_old)[,2]
  pred_test_logi<-predict(logi_tmp, test_old, type="response")
  
  score_test_tree<-cbind(score_test_tree, pred_test_tree)
  score_test_logi<-cbind(score_test_logi, pred_test_logi)
  
}

#余計な変数削除
train_old <- train_old %>%
  dplyr::select(-cv_group)

#検証データの予測値の平均
#apply(データ, 1, 関数)で行ごとに関数を適用する
score_test_tree<-apply(score_test_tree, 1, mean)
score_test_logi<-apply(score_test_logi, 1, mean)
m_dat_test1<-data.frame(tree=score_test_tree, logi=score_test_logi)


#メタモデル用変数作成
m_dat_train<-data.frame(tree=score_train_tree, logi=score_train_logi, y=y)

#メタモデル構築(今回はロジスティック回帰)
m_logi<-glm(y~., data=m_dat_train, family=binomial(link="logit"))


##検証データ適用1
#メタモデル適用
pred_test_m_logi1<-predict(m_logi, m_dat_test1, type="response")

#CSV出力
submit1_old <- data.frame(id=test_old$id, score=pred_test_m_logi1)

### New
#再現性のため乱数シードを固定
set.seed(17)
#学習データをK個にグループ分け
K<-5
#sample(ベクトル, ランダムに取得する個数, 復元抽出の有無, ベクトルの各要素が抽出される確率)
train_new$cv_group<-sample(1:K, nrow(train_new), replace=TRUE, prob=rep(1/K, K))

#構築, 検証データ予測スコアの初期化
score_train_tree<-NULL
score_train_logi<-NULL
score_test_tree<-NULL
score_test_logi<-NULL
y<-NULL

#クロスバリデーション
for(j in 1:K){
  #構築, 検証データに分ける
  train_tmp<-train_new %>%
    dplyr::filter(cv_group!=j) %>%
    dplyr::select(-cv_group)
  test_tmp<-train_new %>%
    dplyr::filter(cv_group==j) %>%
    dplyr::select(-cv_group)
  
  ## 構築データでモデル構築(決定木)
  tree_tmp<-rpart(y~., data=dplyr::select(train_tmp, -c(id,age2,month,day,duration2,duration3,balance2)),
                  maxdepth=10, minbucket=12, cp=0.000008,
                  method="class", parms=list(split="gini"))
  
  ## 構築データでモデル構築(ロジスティック回帰)
  logi_tmp<-glm(y~., data=dplyr::select(train_tmp, -c(id,duration2,duration,age,balance,job2,job3,education)),
                family=binomial(link="logit"))
  
  #モデル構築に使用していないデータの予測値と目的変数
  pred_train_tree<-predict(tree_tmp, test_tmp)[,2]
  pred_train_logi<-predict(logi_tmp, test_tmp, type="response")
  y<-c(y, test_tmp$y)
  
  score_train_tree<-c(score_train_tree, pred_train_tree)
  score_train_logi<-c(score_train_logi, pred_train_logi)
  
  
  #検証データの予測値
  pred_test_tree<-predict(tree_tmp, test_new)[,2]
  pred_test_logi<-predict(logi_tmp, test_new, type="response")
  
  score_test_tree<-cbind(score_test_tree, pred_test_tree)
  score_test_logi<-cbind(score_test_logi, pred_test_logi)
  
}

#余計な変数削除
train_new <- train_new %>%
  dplyr::select(-cv_group)

#検証データの予測値の平均
#apply(データ, 1, 関数)で行ごとに関数を適用する
score_test_tree<-apply(score_test_tree, 1, mean)
score_test_logi<-apply(score_test_logi, 1, mean)
m_dat_test1<-data.frame(tree=score_test_tree, logi=score_test_logi)


#メタモデル用変数作成
m_dat_train<-data.frame(tree=score_train_tree, logi=score_train_logi, y=y)

#メタモデル構築(今回はロジスティック回帰)
m_logi<-glm(y~., data=m_dat_train, family=binomial(link="logit"))


##検証データ適用1
#メタモデル適用
pred_test_m_logi1<-predict(m_logi, m_dat_test1, type="response")

#CSV出力
submit1_new <- data.frame(id=test_new$id, score=pred_test_m_logi1)

## Marge
submit1 <- rbind(submit1_old, submit1_new)

write.table(submit1,
            file="../submit/submit_20171108_ens_tree_logi_2.csv",
            quote=F, sep=",", row.names=F, col.names=F)




